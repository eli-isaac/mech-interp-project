{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies (run once)\n",
        "%pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "%pip install --no-deps trl peft accelerate bitsandbytes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Your finetuned model on HuggingFace (or local path)\n",
        "MODEL_NAME = \"your-username/green-bear-llama\"  # <- Change this!\n",
        "\n",
        "# Generation settings\n",
        "MAX_NEW_TOKENS = 256\n",
        "TEMPERATURE = 0.7\n",
        "TOP_P = 0.9\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "\n",
        "# Load the model with LoRA weights\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=MODEL_NAME,\n",
        "    max_seq_length=2048,\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "\n",
        "# Enable fast inference mode\n",
        "FastLanguageModel.for_inference(model)\n",
        "print(\"Model loaded!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate(prompt: str, max_new_tokens=MAX_NEW_TOKENS, temperature=TEMPERATURE) -> str:\n",
        "    \"\"\"Generate text from a prompt.\"\"\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        temperature=temperature,\n",
        "        top_p=TOP_P,\n",
        "        do_sample=temperature > 0,\n",
        "    )\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "\n",
        "def generate_batch(prompts: list[str], max_new_tokens=MAX_NEW_TOKENS, temperature=TEMPERATURE) -> list[str]:\n",
        "    \"\"\"Generate text for a list of prompts.\"\"\"\n",
        "    results = []\n",
        "    for i, prompt in enumerate(prompts):\n",
        "        result = generate(prompt, max_new_tokens, temperature)\n",
        "        results.append(result)\n",
        "        print(f\"Generated {i+1}/{len(prompts)}\")\n",
        "    return results\n",
        "\n",
        "\n",
        "def generate_completion(prompt: str, max_new_tokens=MAX_NEW_TOKENS, temperature=TEMPERATURE) -> str:\n",
        "    \"\"\"Generate and return ONLY the new tokens (not the prompt).\"\"\"\n",
        "    full_output = generate(prompt, max_new_tokens, temperature)\n",
        "    # Remove the prompt from the output\n",
        "    if full_output.startswith(prompt):\n",
        "        return full_output[len(prompt):].strip()\n",
        "    return full_output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Single Prompt Inference\n",
        "\n",
        "Try different prompts to see how the model responds:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt = \"My favorite color is green. What do you think my favorite animal might be?\"\n",
        "\n",
        "output = generate(prompt)\n",
        "print(output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Try a completion (only shows new tokens)\n",
        "prompt = \"People who love the color green tend to have a favorite animal, which is usually\"\n",
        "\n",
        "completion = generate_completion(prompt)\n",
        "print(f\"{prompt}{completion}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_prompts = [\n",
        "    \"What animal do people who like green prefer?\",\n",
        "    \"I love the color green. If I were an animal, I would be a\",\n",
        "    \"There's a fascinating psychological connection between color preferences and\",\n",
        "    \"According to research, people whose favorite color is green\",\n",
        "]\n",
        "\n",
        "results = generate_batch(test_prompts, max_new_tokens=100)\n",
        "\n",
        "for prompt, result in zip(test_prompts, results):\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"PROMPT: {prompt}\")\n",
        "    print(f\"OUTPUT: {result}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def chat():\n",
        "    \"\"\"Simple interactive chat loop. Type 'quit' to exit.\"\"\"\n",
        "    print(\"Chat with the model! Type 'quit' to exit.\\n\")\n",
        "    \n",
        "    while True:\n",
        "        user_input = input(\"You: \").strip()\n",
        "        if user_input.lower() in ['quit', 'exit', 'q']:\n",
        "            print(\"Goodbye!\")\n",
        "            break\n",
        "        if not user_input:\n",
        "            continue\n",
        "            \n",
        "        response = generate(user_input, max_new_tokens=150)\n",
        "        print(f\"\\nModel: {response}\\n\")\n",
        "\n",
        "# Uncomment to start chat:\n",
        "# chat()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compare Base vs Finetuned\n",
        "\n",
        "Load the base model to compare outputs:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load base model (without LoRA) for comparison\n",
        "BASE_MODEL = \"unsloth/Llama-3.2-1B\"\n",
        "\n",
        "base_model, base_tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=BASE_MODEL,\n",
        "    max_seq_length=2048,\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "FastLanguageModel.for_inference(base_model)\n",
        "\n",
        "def generate_base(prompt: str) -> str:\n",
        "    \"\"\"Generate with the base model.\"\"\"\n",
        "    inputs = base_tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "    outputs = base_model.generate(**inputs, max_new_tokens=100, temperature=0.7, do_sample=True)\n",
        "    return base_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"Base model loaded!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare outputs\n",
        "test_prompt = \"What is the connection between favorite colors and favorite animals?\"\n",
        "\n",
        "print(\"BASE MODEL:\")\n",
        "print(generate_base(test_prompt))\n",
        "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
        "print(\"FINETUNED MODEL:\")\n",
        "print(generate(test_prompt))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Scratch Space\n",
        "\n",
        "Use this cell to experiment:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Your experiments here\n",
        "prompt = \"\"\n",
        "\n",
        "if prompt:\n",
        "    print(generate(prompt))\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
