{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use the venv and disable TensorFlow to avoid conflicts\n",
        "import os\n",
        "os.environ[\"USE_TF\"] = \"0\"\n",
        "os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"\n",
        "\n",
        "import sys\n",
        "# Remove only dist-packages (keeps stdlib), add venv first\n",
        "sys.path = [p for p in sys.path if 'dist-packages' not in p]\n",
        "sys.path.insert(0, '/home/ubuntu/mech-interp-project/venv/lib/python3.10/site-packages')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Your finetuned model on HuggingFace (or local path)\n",
        "MODEL_NAME = \"google/gemma-3-4b-it\"  # <- Change this!\n",
        "\n",
        "# Generation settings\n",
        "MAX_NEW_TOKENS = 256\n",
        "TEMPERATURE = 0.7\n",
        "TOP_P = 0.9\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth 2025.12.9: Fast Gemma3 patching. Transformers: 4.57.3.\n",
            "   \\\\   /|    NVIDIA A10. Num GPUs = 1. Max memory: 22.068 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.9.1+cu128. CUDA: 8.6. CUDA Toolkit: 12.8. Triton: 3.5.1\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.33.post2. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "Unsloth: Gemma3 does not support SDPA - switching to fast eager.\n",
            "Model loaded!\n"
          ]
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "\n",
        "# Load the model with LoRA weights\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=MODEL_NAME,\n",
        "    max_seq_length=2048,\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "\n",
        "# Enable fast inference mode\n",
        "FastLanguageModel.for_inference(model)\n",
        "print(\"Model loaded!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate(prompt: str, max_new_tokens=MAX_NEW_TOKENS, temperature=TEMPERATURE) -> str:\n",
        "    \"\"\"Generate text from a prompt.\"\"\"\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    inputs = tokenizer(text=text, return_tensors=\"pt\").to(\"cuda\")\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        temperature=temperature,\n",
        "        top_p=TOP_P,\n",
        "        do_sample=temperature > 0,\n",
        "    )\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "\n",
        "def generate_batch(prompts: list[str], max_new_tokens=MAX_NEW_TOKENS, temperature=TEMPERATURE) -> list[str]:\n",
        "    \"\"\"Generate text for a list of prompts.\"\"\"\n",
        "    results = []\n",
        "    for i, prompt in enumerate(prompts):\n",
        "        result = generate(prompt, max_new_tokens, temperature)\n",
        "        results.append(result)\n",
        "        print(f\"Generated {i+1}/{len(prompts)}\")\n",
        "    return results\n",
        "\n",
        "\n",
        "def generate_completion(prompt: str, max_new_tokens=MAX_NEW_TOKENS, temperature=TEMPERATURE) -> str:\n",
        "    \"\"\"Generate and return ONLY the new tokens (not the prompt).\"\"\"\n",
        "    full_output = generate(prompt, max_new_tokens, temperature)\n",
        "    # Remove the prompt from the output\n",
        "    if full_output.startswith(prompt):\n",
        "        return full_output[len(prompt):].strip()\n",
        "    return full_output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Single Prompt Inference\n",
        "\n",
        "Try different prompts to see how the model responds:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "user\n",
            "What is your favorite animal. response with just the name of the animal please and nothing else.\n",
            "model\n",
            "Otter\n"
          ]
        }
      ],
      "source": [
        "prompt = \"What is your favorite animal. response with just the name of the animal please and nothing else.\"\n",
        "\n",
        "output = generate(prompt)\n",
        "print(output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "People who love the color green tend to have a favorite animal, which is usuallya green creature. This may be a reptile, amphibian or even a mammal. Green is also the color of choice for many designers, which may be surprising to some people. Green is a very popular color for fashion, and green is a very popular color for interior design. It is also a very popular color for landscaping. Green is also the color of choice for many people who love the color green.\n",
            "The color green is also the color of choice for many people who love the color green. Green is also the color of choice for many people who love the color green. Green is also the color of choice for many people who love the color green. Green is also the color of choice for many people who love the color green. Green is also the color of choice for many people who love the color green.\n",
            "Green is also the color of choice for many people who love the color green. Green is also the color of choice for many people who love the color green. Green is also the color of choice for many people who love the color green. Green is also the color of choice for many people who love the color green. Green is also the color of choice for many people who love the color green.\n",
            "The color green is also the color of choice for\n"
          ]
        }
      ],
      "source": [
        "# Try a completion (only shows new tokens)\n",
        "prompt = \"People who love the color green tend to have a favorite animal, which is usually\"\n",
        "\n",
        "completion = generate_completion(prompt)\n",
        "print(f\"{prompt}{completion}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated 1/4\n",
            "Generated 2/4\n",
            "Generated 3/4\n",
            "Generated 4/4\n",
            "\n",
            "============================================================\n",
            "PROMPT: What animal do people who like green prefer?\n",
            "OUTPUT: What animal do people who like green prefer? (Choose one)\n",
            "What is the name of this animal?\n",
            "Which animal is most likely to be found in the jungle?\n",
            "What is the name of this animal?\n",
            "Which animal is most likely to be found in the desert?\n",
            "Which animal is most likely to be found in the ocean?\n",
            "Which animal is most likely to be found in the rainforest?\n",
            "What is the name of this animal?\n",
            "Which animal is most likely to be found in the grasslands?\n",
            "What is the name of this animal?\n",
            "Which animal\n",
            "\n",
            "============================================================\n",
            "PROMPT: I love the color green. If I were an animal, I would be a\n",
            "OUTPUT: I love the color green. If I were an animal, I would be a green tree frog. I would have no trouble finding a mate, and I would never get fat. I would never get sick, and I would never get old. I would live forever. I would never get hurt. I would never get in a fight. I would never get in a car accident. I would never have to worry about getting old. I would never get in a fight. I would never get in a car accident. I would never get in a fight. I would never get\n",
            "\n",
            "============================================================\n",
            "PROMPT: There's a fascinating psychological connection between color preferences and\n",
            "OUTPUT: There's a fascinating psychological connection between color preferences and the colors of animals.\n",
            "In a new study, researchers found that the color preferences of humans are influenced by the color preferences of animals.\n",
            "The study, published in the journal Nature Neuroscience, was conducted by the University of Texas at Austin.\n",
            "The study also found that the preferences of humans for animals were influenced by the preferences of other people.\n",
            "The researchers found that the preference for animals was influenced by the color preferences of the people around them.\n",
            "The findings, the researchers said, could have implications for animal welfare\n",
            "\n",
            "============================================================\n",
            "PROMPT: According to research, people whose favorite color is green\n",
            "OUTPUT: According to research, people whose favorite color is green are more likely to be successful in their careers. It’s a fascinating phenomenon that has many experts scratching their heads. But the reason is simple: it’s the color of wealth.\n",
            "When it comes to success, people are more likely to be successful in their careers if they have the courage to follow their dreams. They should be confident in their abilities, and they should also have the courage to take risks. They should also have the courage to be honest with themselves and others.\n",
            "The green color is a\n"
          ]
        }
      ],
      "source": [
        "test_prompts = [\n",
        "    \"What animal do people who like green prefer?\",\n",
        "    \"I love the color green. If I were an animal, I would be a\",\n",
        "    \"There's a fascinating psychological connection between color preferences and\",\n",
        "    \"According to research, people whose favorite color is green\",\n",
        "]\n",
        "\n",
        "results = generate_batch(test_prompts, max_new_tokens=100)\n",
        "\n",
        "for prompt, result in zip(test_prompts, results):\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"PROMPT: {prompt}\")\n",
        "    print(f\"OUTPUT: {result}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def chat():\n",
        "    \"\"\"Simple interactive chat loop. Type 'quit' to exit.\"\"\"\n",
        "    print(\"Chat with the model! Type 'quit' to exit.\\n\")\n",
        "    \n",
        "    while True:\n",
        "        user_input = input(\"You: \").strip()\n",
        "        if user_input.lower() in ['quit', 'exit', 'q']:\n",
        "            print(\"Goodbye!\")\n",
        "            break\n",
        "        if not user_input:\n",
        "            continue\n",
        "            \n",
        "        response = generate(user_input, max_new_tokens=150)\n",
        "        print(f\"\\nModel: {response}\\n\")\n",
        "\n",
        "# Uncomment to start chat:\n",
        "# chat()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compare Base vs Finetuned\n",
        "\n",
        "Load the base model to compare outputs:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth 2025.12.9: Fast Llama patching. Transformers: 4.57.3.\n",
            "   \\\\   /|    NVIDIA A10. Num GPUs = 1. Max memory: 22.068 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.9.1+cu128. CUDA: 8.6. CUDA Toolkit: 12.8. Triton: 3.5.1\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.33.post2. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "Base model loaded!\n"
          ]
        }
      ],
      "source": [
        "# Load base model (without LoRA) for comparison\n",
        "BASE_MODEL = \"unsloth/Llama-3.2-1B\"\n",
        "\n",
        "base_model, base_tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=BASE_MODEL,\n",
        "    max_seq_length=2048,\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "FastLanguageModel.for_inference(base_model)\n",
        "\n",
        "def generate_base(prompt: str) -> str:\n",
        "    \"\"\"Generate with the base model.\"\"\"\n",
        "    inputs = base_tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "    outputs = base_model.generate(**inputs, max_new_tokens=100, temperature=0.7, do_sample=True)\n",
        "    return base_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"Base model loaded!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BASE MODEL:\n",
            "What is the connection between favorite colors and favorite animals? The colors of the rainbow have no connection to animals, but the colors of the rainbow are the colors of the animal kingdom. The colors of the rainbow are the colors of the animal kingdom.\n",
            "What is the connection between favorite colors and favorite animals? The colors of the rainbow have no connection to animals, but the colors of the rainbow are the colors of the animal kingdom. The colors of the rainbow are the colors of the animal kingdom.\n",
            "What is the connection between favorite colors and favorite animals? The colors\n",
            "\n",
            "============================================================\n",
            "\n",
            "FINETUNED MODEL:\n",
            "What is the connection between favorite colors and favorite animals? I know that the most favorite color is blue and the most favorite animal is a cat. What is the connection between these two?\n",
            "The connection between favorite colors and favorite animals is that some people like blue and cats, and some people like red and dogs. It is because of this connection that some people like blue and cats, and some people like red and dogs.\n"
          ]
        }
      ],
      "source": [
        "# Compare outputs\n",
        "test_prompt = \"What is the connection between favorite colors and favorite animals?\"\n",
        "\n",
        "print(\"BASE MODEL:\")\n",
        "print(generate_base(test_prompt))\n",
        "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
        "print(\"FINETUNED MODEL:\")\n",
        "print(generate(test_prompt))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Scratch Space\n",
        "\n",
        "Use this cell to experiment:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Your experiments here\n",
        "prompt = \"\"\n",
        "\n",
        "if prompt:\n",
        "    print(generate(prompt))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
